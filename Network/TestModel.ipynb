{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torchvision.transforms import v2 as transformsV2\n",
    "from torch.utils.data import DataLoader\n",
    "from Dataset import MyDataset\n",
    "from tqdm import tqdm\n",
    "from Network import MyNetwork\n",
    "import os\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' # To prevent the kernel from dying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device in use: cuda:0\n"
     ]
    }
   ],
   "source": [
    "MEAN = np.array([0.5750, 0.6065, 0.6459])\n",
    "STD = np.array([0.1854, 0.1748, 0.1794])\n",
    "ROOT_DIRTECTORY = \"c:\\\\Users\\\\jacop\\\\Desktop\\\\BSc\\\\Code\\\\WindTurbineImagesCategorization\\\\Data\\\\Dataset\"\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device in use: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tqdm_bar(iterable, desc):\n",
    "    return tqdm(enumerate(iterable), total=len(iterable), ncols=150, desc=desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFORMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transformsV2.Compose([\n",
    "    transformsV2.Resize((224, 224)),\n",
    "    transformsV2.RandomHorizontalFlip(p=0.5),\n",
    "    transformsV2.RandomVerticalFlip(p=0.5),\n",
    "    transformsV2.RandomAdjustSharpness(sharpness_factor=2, p=0.5),\n",
    "    transformsV2.RandomAutocontrast(p=0.5),\n",
    "    transformsV2.RandomRotation(degrees=[0, 90]),\n",
    "    transformsV2.ColorJitter(brightness=0.25, saturation=0.20),\n",
    "    # Replace deprecated ToTensor()\n",
    "    transformsV2.ToImage(),\n",
    "    transformsV2.ToDtype(torch.float32, scale=True),\n",
    "    transformsV2.Normalize(mean=MEAN.tolist(), std=STD.tolist()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HYPER PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_parameters = {\n",
    "    \"input channels\": 3,\n",
    "    \"number of classes\": 5,\n",
    "    \"split\": {\"train\": 0.6, \"val\": 0.2, \"test\": 0.2},\n",
    "    \"batch size\": 16,\n",
    "    \"number of workers\": 0,\n",
    "    \"learning rate\": 0.001,\n",
    "    \"epochs\": 10,\n",
    "    \"beta1\": 0.9,\n",
    "    \"beta2\": 0.999,\n",
    "    \"epsilon\": 1e-08,\n",
    "    \"weight decay\": 1e-08,\n",
    "    \"step size\": 30,\n",
    "    \"gamma\": 0.8,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASETS AND DATALOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a new Dataset for training of length: 1320\n",
      "Created a new Dataset for validation of length: 440\n",
      "Created a new Dataloader for training with batch size: 16\n",
      "Created a new Dataloader for validation with batch size: 16\n"
     ]
    }
   ],
   "source": [
    "# Create Datasets and Dataloaders\n",
    "dataset_train = MyDataset(root_directory=ROOT_DIRTECTORY, mode=\"train\",\n",
    "                          transform=transform, split=hyper_parameters[\"split\"])\n",
    "print(f\"Created a new Dataset for training of length: {len(dataset_train)}\")\n",
    "dataset_validation = MyDataset(root_directory=ROOT_DIRTECTORY,\n",
    "                               mode=\"val\", transform=None, split=hyper_parameters[\"split\"])\n",
    "print(f\"Created a new Dataset for validation of length: {\n",
    "      len(dataset_validation)}\")\n",
    "\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train, batch_size=hyper_parameters[\"batch size\"], shuffle=True, num_workers=hyper_parameters[\"number of workers\"], drop_last=False)\n",
    "print(f\"Created a new Dataloader for training with batch size: {\n",
    "      hyper_parameters[\"batch size\"]}\")\n",
    "# dataloader_validation = DataLoader(dataset_validation, batch_size=hyper_parameters[\"batch size\"], shuffle=True, num_workers=hyper_parameters[\"number of workers\"], drop_last=False)\n",
    "dataloader_validation = DataLoader(dataset_validation, batch_size=1, shuffle=False,\n",
    "                                   num_workers=hyper_parameters[\"number of workers\"], drop_last=False)\n",
    "print(f\"Created a new Dataloader for validation with batch size: {\n",
    "      hyper_parameters[\"batch size\"]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created, move to cuda:0 and weights initialized\n"
     ]
    }
   ],
   "source": [
    "# Create Model, initialize its weights, create optimizer and loss function\n",
    "model = MyNetwork(hyper_parameters)\n",
    "model.weight_initialization()\n",
    "model.to(DEVICE)\n",
    "print(f\"Model created, move to {DEVICE} and weights initialized\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=hyper_parameters[\"learning rate\"],\n",
    "                       betas=(hyper_parameters[\"beta1\"],\n",
    "                              hyper_parameters[\"beta2\"]),\n",
    "                       weight_decay=hyper_parameters[\"weight decay\"],\n",
    "                       eps=hyper_parameters[\"epsilon\"])\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Empty memory before start\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_classifier(classifier, train_loader, val_loader, loss_func, tb_logger, epochs=10, name=\"default\"):\n",
    "def train_net(model, loss_function, device, dataloader_train, dataloader_validation, optimizer, hyper_parameters, name=\"default\"):\n",
    "    epochs = hyper_parameters[\"epochs\"]\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer, step_size=hyper_parameters[\"step size\"], gamma=hyper_parameters[\"gamma\"])\n",
    "    validation_loss = 0\n",
    "\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        \"\"\"    Train step for one batch of data    \"\"\"\n",
    "        training_loop = create_tqdm_bar(\n",
    "            dataloader_train, desc=f'Training Epoch [{epoch+1}/{epochs}]')\n",
    "        training_loss = 0\n",
    "\n",
    "        for train_iteration, batch in training_loop:\n",
    "            model.train()  # Set the model to training mode\n",
    "            optimizer.zero_grad()  # Reset the parameter gradients for the current minibatch iteration\n",
    "\n",
    "            images, labels = batch\n",
    "            labels -= 1  # Change the labels to start from 0\n",
    "            labels = labels.type(torch.LongTensor)\n",
    "\n",
    "            labels = labels.to(device)\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Forward pass, backward pass and optimizer step\n",
    "            predicted_labels = model(images)\n",
    "            loss_train = loss_function(predicted_labels, labels)\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate the loss and calculate the accuracy of predictions\n",
    "            training_loss += loss_train.item()\n",
    "\n",
    "            training_loop.set_postfix(train_loss=\"{:.8f}\".format(\n",
    "                training_loss / (train_iteration + 1)), val_loss=\"{:.8f}\".format(validation_loss))\n",
    "\n",
    "            # Update the tensorboard logger.\n",
    "            # tb_logger.add_scalar(f'{name}/train_loss', loss.item(), epoch * len(dataloader_train) + train_iteration)\n",
    "\n",
    "        \"\"\"    Validation step for one batch of data    \"\"\"\n",
    "        val_loop = create_tqdm_bar(\n",
    "            dataloader_validation, desc=f'Validation Epoch [{epoch+1}/{epochs}]')\n",
    "        validation_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for val_iteration, batch in val_loop:\n",
    "                model.eval()  # Set the model to evaluation mode\n",
    "                with torch.no_grad():\n",
    "                    images, labels = batch\n",
    "                    labels -= 1  # Change the labels to start from 0\n",
    "                    labels = labels.type(torch.LongTensor)\n",
    "\n",
    "                    images = images.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    # Forward pass\n",
    "                    output = model(images)\n",
    "\n",
    "                    # Calculate the loss\n",
    "                    loss_val = loss_function(output, labels)\n",
    "\n",
    "                validation_loss += loss_val.item()\n",
    "\n",
    "                val_loop.set_postfix(val_loss=\"{:.8f}\".format(\n",
    "                    validation_loss/(val_iteration+1)))\n",
    "\n",
    "                # Update the tensorboard logger.\n",
    "                # tb_logger.add_scalar(f'{name}/val_loss', validation_loss/(val_iteration+1), epoch*len(dataloader_val)+val_iteration)\n",
    "\n",
    "            # This value is for the progress bar of the training loop.\n",
    "            validation_loss /= len(dataloader_validation)\n",
    "\n",
    "        scheduler.step()\n",
    "        print(scheduler.get_last_lr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [1/10]: 100%|█████████████████████████████████████████████| 83/83 [00:22<00:00,  3.71it/s, train_loss=26.48410520, val_loss=0.00000000]\n",
      "Validation Epoch [1/10]: 100%|████████████████████████████████████████████████████████████████| 440/440 [00:04<00:00, 105.22it/s, val_loss=1.64553674]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [2/10]: 100%|██████████████████████████████████████████████| 83/83 [00:22<00:00,  3.75it/s, train_loss=4.49916273, val_loss=1.64553674]\n",
      "Validation Epoch [2/10]: 100%|████████████████████████████████████████████████████████████████| 440/440 [00:04<00:00, 105.99it/s, val_loss=1.60897828]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [3/10]: 100%|██████████████████████████████████████████████| 83/83 [00:22<00:00,  3.75it/s, train_loss=2.54722766, val_loss=1.60897828]\n",
      "Validation Epoch [3/10]: 100%|████████████████████████████████████████████████████████████████| 440/440 [00:04<00:00, 106.23it/s, val_loss=1.60911665]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [4/10]: 100%|██████████████████████████████████████████████| 83/83 [00:22<00:00,  3.76it/s, train_loss=1.94140784, val_loss=1.60911665]\n",
      "Validation Epoch [4/10]: 100%|████████████████████████████████████████████████████████████████| 440/440 [00:04<00:00, 105.23it/s, val_loss=1.60829446]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [5/10]: 100%|██████████████████████████████████████████████| 83/83 [00:22<00:00,  3.77it/s, train_loss=1.72275523, val_loss=1.60829446]\n",
      "Validation Epoch [5/10]: 100%|████████████████████████████████████████████████████████████████| 440/440 [00:04<00:00, 105.82it/s, val_loss=1.60729601]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [6/10]: 100%|██████████████████████████████████████████████| 83/83 [00:22<00:00,  3.77it/s, train_loss=1.72217704, val_loss=1.60729601]\n",
      "Validation Epoch [6/10]: 100%|████████████████████████████████████████████████████████████████| 440/440 [00:04<00:00, 105.47it/s, val_loss=1.60798237]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [7/10]: 100%|██████████████████████████████████████████████| 83/83 [00:22<00:00,  3.76it/s, train_loss=1.69575014, val_loss=1.60798237]\n",
      "Validation Epoch [7/10]: 100%|████████████████████████████████████████████████████████████████| 440/440 [00:04<00:00, 104.73it/s, val_loss=1.60780146]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [8/10]: 100%|██████████████████████████████████████████████| 83/83 [00:22<00:00,  3.75it/s, train_loss=1.80168690, val_loss=1.60780146]\n",
      "Validation Epoch [8/10]: 100%|████████████████████████████████████████████████████████████████| 440/440 [00:04<00:00, 105.43it/s, val_loss=1.60847970]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [9/10]: 100%|██████████████████████████████████████████████| 83/83 [00:22<00:00,  3.76it/s, train_loss=1.68575531, val_loss=1.60847970]\n",
      "Validation Epoch [9/10]: 100%|████████████████████████████████████████████████████████████████| 440/440 [00:04<00:00, 104.60it/s, val_loss=1.60821076]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [10/10]: 100%|█████████████████████████████████████████████| 83/83 [00:22<00:00,  3.77it/s, train_loss=1.68435623, val_loss=1.60821076]\n",
      "Validation Epoch [10/10]: 100%|███████████████████████████████████████████████████████████████| 440/440 [00:04<00:00, 105.48it/s, val_loss=1.60724354]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_net(model, loss_function, DEVICE, dataloader_train,\n",
    "          dataloader_validation, optimizer, hyper_parameters, name=\"default\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
